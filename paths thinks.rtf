{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh14000\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 fixed\
\
===\
the value of xpos should be [xx ] (validated between 0 and screenwidth )\
\
=\
the value of ypos should be [xx] (validated between 0 an screenheight )\
\
=\
\
the value of opacity should be [xx ] (validated between 1 and 100)\
\
====\
\
vflat\
\
===\
count [fromleft/fromright]\
as value of velocity varies from [ x1] or below to [x2] or above, xpos should vary between [ x3] and [x4] pixels from left/right\
\
x1 validated as < x2 >=0 < 127\
x2 validated as >x1 >0 <=127\
x3 validated as <x4 >=0 < screenwidth\
\
=\
[fade/emerge]\
as value of velocity varies from [ x1] or below to [x2] or above, opacity of image should vary between [x3]  and [x4] (in an inverse fashion)\
\
x1 validated as < x2 >=0 < 127\
x2 validated as >x1 >0 <=127\
x3 validated as <x4 >=0 < 100 \
note that whether fade or emerge, x3 is always less than x4\
\
====\
\
sweep\
===\
\
I mean you could constrain the input times ie no changes before time 1, none after time 2 but I don\'92t *think* that would be useful\
\
[from left/right]\
as value of time varies from 0 to [x1] xpos should vary from [x2] to [x3] pixels from left/right\
\
x1 is validated as somewhere between 1/framerate and \'85 oh I don\'92t know 20s\
x2 validated as <x3 >=0 < screenwidth\
x3 is validated as >x2 >0 <=screenwidth\
\
=\
\
[fade/emerge]\
as value of time varies from 0 to [x1] opacity should vary from [x2 ] to [x3] (in an inverse fashion)\
\
====\
\
vfall\
===\
\
[from left/right]\
as velocity varies from [x1] or below to [x2] or above, the starting position of xpos should vary \
\
====\
\
accel\
===\
\
as s = ut +(at^2)/2\
\
We need to know u ( initial velocity) t time a acceleration\
that gives us S displacement, but we also need to know initial value. We also need to say whether s is displacement from max or from min. That\'92s separate from whether a is negative\
\
probably it\'92s OK to use omin as start pos and omax as some kind of output scaling. time period would indicate u in some way. Secondary time could iindicate acceleration. So stilll enough data. maybe. what if we decide to base u on input velocity? wouldn\'92t that mean the normal imin, imax, omin, imax all to handle u? then where would \
\
what next? wobble? Acceleration? \
Record performance?\
Polyphonic?\
Smear?\
\
I think forget about extra features, concentrate on \
\
recording performances and\
export to, dunno pencil2d format or mp4? whatever is easier I guess\
\
OK big technical rabbit hole. To save images to avi, mp4 or whatever, you need the images in opencv format, not qpixmap.\
\
as all manipulation was (successfully) being done in qpixmap, thought I\'92d just convert to cv before writing \
\
couldn\'92t find any way of converting qpixmap to cv, but going the other way seemed a doddle. Moreover, everyone seemed to be saying \'91why do you want to do that?\'92 as cv is like the premier image manipulation format and qt is just about display. \
\
Looked at doing the loading and manipulation in cv format. Not as straightforward in cv, mostly because, being more flexible, it\'92s more low-level. You are exposed much more to the matrices that hold the image data.\
\
Just doing a straight opaque overlay required quite a bit of research. But in the testrig, I managed to achieve scaling, positioning and opacity of overlays.\
\
Moving the functions from the testrig to the main prog, it didn\'92t work. The error thrown was \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\fs22 \cf2 \CocoaLigature0 ValueError: operands could not be broadcast together with shapes (540,174,4) (540,174,3)\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \CocoaLigature1 \
Which didn\'92t mean a lot to me. So I look into how numpy handles matrices. It talked about the shape of the matrices and \'91dot products\'92 at which point I say \'93I wasn\'92t in when we did matrices miss\'94.\
\
So I read about matrices and dot products and how you can\'92t produce a dot product on matrices with incompatible shapes. And now I look at it, those shapes are different.\
\
So why are the shapes different. Turns out that the first two dimensions are, fairly unsurprisingly, the width and height. In the case of a greyscale, there are only those two dimensions. For colour images there is a third dimension, usually of size 3 for RG and B bitmaps. If the size of the 3rd dimension is 4, the last one is transparency.\
\
jpeg images don\'92t seem to have a transparency layer, but png images do. So if you\'92re reading a mixture of png and jpeg, there\'92s a possibility you\'92ll get incompatible shapes. But it shouldn\'92t be a problem.\
\
If use use the switch cv2.IMREAD_GREYSCALE you will only ever get an x*y matrix\
If you use the switch cv2.IMREAD_COLOR you will only ever get an x*y*3 matrix\
if you use the switch cv2.IMREAD_UNCHANGED you might get an x*y matrix if it was natively monochrome. You might get an x*y*3 matrix from a jpeg or an x*y*4 from a png\
\
Now it seems that the overlay routine can cope with overlaying an x*y*4 png on an x*y*3 background, but it can\'92t cope with overlaying an x*y*3 on an x*y*4 background.\
\
Expanding the layers window and some primitive transport control\
\
Add a 17th row with no voice select and showing the audio waveform in \
\
The new column which shows kinda what performance activity there is or what audio there is. At the basic level, you can see what tracks are clean and which ones have performance activity. \
\
Ultimately you\'92d want good synchronisation between the waveform and performance graphics and the ability to place the transport at a particular point and represent the current image state at that point. \
\
frame, beat or second graticule. Ugh! do we respond in clever ways to tempo change or just outlaw them?\
\
More sophisticated midi saving which would involve storing the current state at the end of a performance, but only overwriting the original with the temp file at \'92user save\'92 time.\
\
Update the temp performance file if a track is cleaned out\
\
initially only allow record on clean tracks \
\
how to make the midi/performance timelines\
Is the solution to this similar to the jump-to-frame/time problem?\
Brute force solution to jtf would be to keep a massive array containing one soundboard for every frame for the entire duration\
Maybe there could be more sparse \'91milestone\'92 soundboards created at fixed frame numbers (? one every 50?) and position finely by rolling forward, adding or subtracting from the milestone.\
\
In any case we need to be able to roll the player in \'91asap\'92 time rather than real time, making soundboards, but not rendering them\
\
To draw lines on the timelines, it would only be necessary to have an array of starts and\'85 let\'92s face it, it\'92s just a soundboard. Its generation would be similar, but the action on a note_off/note_on, velocity 0 would be more complex.\
\
Lines could be positioned according to note value and coloured according to velocity.\
\
Both jtf and timeline rendering would involve running the player in asap time with a different output from normal.\
\
###\
\
need to make transportStop stop everything. Currently it only stops the music playback and does some jiggery pokery with saving the midi data.\
\
need to add stop the player (the thing that pushes midi file data to the board)\
 call player::stop()  (self.p.stop()) \
the listener (that pushes live perf data to the board)\
  set the self.stop flag\
The camera (the thing that sends the rendering of the board to the theatre.\
  set the self.stop flag\
\
Get the export to mp4 progress window to work - ! most of the way there. What a pain.\
\
We need to have the performance as a memory thing, and not just something referred to by the original filename. At present, we have routines that get tempo info and, more importantly make timeline images that work from the original file rather than a memory image.\
\
Need to get to a state where we only call MidiFile at project load/ midi import time and at all other times work on a memory image.\
\
\
Need to get \'91recorded\'92 performance showing on the layer table\
Need to be able to save recorded data with the existing data\
\
In order to hit the next target, we need to store track index info along with the other layer info\
\
need to be able to delete a channel of recorded data\
\
project save and save as with midi\
\
27/3/2022 got here!\
\
implement project new \
\
implemented new and imp/exp of voices, but need work on dirty/clean checks \
\
if no audio, disable:\
\
save, save as, import voice, export voice, new voice, edit voice, layers, import midi\
\
so disable \
savePAct\
savePAsAct\
expVAct\
impVAct\
impMAct\
expVidAct\
\
Ton of testing\
\
Need to look at the qualiity of syncronisation \
\
pause and seek position, punch in. etc -similar to timeline gen and mp4 gen\
\
more paths \
\
image paths\
\
\
\
\
\
\
\
}